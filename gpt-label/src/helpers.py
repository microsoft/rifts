import pandas as pd

from collections import Counter
from itertools import chain

import os
import json

os.environ["OPENAI_API_KEY"] = "YOUR_KEY_HERE"

from openai import AsyncOpenAI

client = AsyncOpenAI()

MERGE_MAP = {
    "next turn": "next turn",
    "response": "next turn",
    "overresponse": "overresponse",
    "acknowledgement": "acknowledgement",
    "clarification": "clarification",
    "followup": "followup",
    "next turn": "next turn",
    "repair": "repair",
    "repeat": "repeat",
    "display": "display",
    "agreement": "next turn",
    "disagreement": "next turn",
    "topic switch": "topic switch",
    "start": "start",
}


def load_dfs():
    fnames = os.listdir("./logs_parquet")

    dfs = []
    for fname in fnames:
        dfs.append(pd.read_parquet(f"./logs_parquet/{fname}"))

    df = pd.concat(dfs)

    df["DomainPrimary"] = df["DomainPrimary"].apply(
        lambda x: x.replace("{", "").replace("}", "") if x is not None else x
    )

    print("Number of interactions START: ")
    print(len(df))

    # filter out domains that appear less than 5% of the time
    domain_counts = df["DomainPrimary"].value_counts(normalize=True)
    df = df[df["DomainPrimary"].isin(domain_counts[domain_counts > 0.01].index)]

    print("Number of interactions AFTER DOMAIN FILTER: ")
    print(len(df))

    with open("labeled_data/copilot_sample_language.json", "r") as f:
        language = json.load(f)

    ids = []
    for k in language:
        if k["labels"]["label"] in ["YES", "yes"]:
            ids.append(k["conversation_id"])

    df = df[df["ConversationId"].isin(ids)]

    print("Number of interactions AFTER LANGUAGE FILTER: ")
    print(len(df))

    df = df[df["NumBotMessages"] >= 1]

    print("Number of interactions AFTER BOT MESSAGE FILTER: ")
    print(len(df))

    df["parsedMessages"] = df["contentChatOnly"].apply(lambda x: get_turns_as_dict(x))

    print("Number of interactions: ")
    print(len(df))

    return df


def get_turns_as_dict(content, user_prefix="User:", ai_prefix="AI:", remove_refs=True):
    # Split the content by newline and get all user/AI turns
    if content.startswith('"') and content.endswith(
        '"'
    ):  # some conversations are in double quotes, remove
        content = content.strip('"')

    lines = content.split("<NL/>")
    current_turn = [lines[0]]
    turns = []

    for line in lines[1:]:
        if line.startswith(user_prefix) or line.startswith(ai_prefix):
            if (
                "Glad you liked this answer!" in line
                or "Thanks for your feedback!" in line
                or "Thanks for the heads up. Please give feedback so the Bing team can take a look."
                in line
            ):
                current_turn.append(line)
            else:
                turns.append(" ".join(current_turn))
                current_turn = [line]
        else:
            current_turn.append(line)

    turns.append(" ".join(current_turn))

    # Mark the beginning of the conversation as the first user turn
    for i, turn in enumerate(turns):
        if turn.startswith(user_prefix):
            turns = turns[i:]
            break

    # Get all search queries generated by the AI during the conversation
    searches = set()
    for turn in turns:
        if turn.startswith(f"{ai_prefix}Searching the web for:"):
            first_index = turn.find("`")  # Find index of first backtick
            second_index = turn.find(
                "`", first_index + 1
            )  # Find index of second backtick
            searches.add(turn[first_index + 1 : second_index])

    output = []
    turn_num = 0
    for turn in turns:
        # If this turn is the AI repeating a search query, discard
        is_search_turn = False
        for search_query in searches:
            if turn == f"{ai_prefix}{search_query}":
                is_search_turn = True
                break

        # Also discard turns where AI says "Searching the web" or "Generating answers"
        if (
            not is_search_turn
            and not turn.startswith(f"{ai_prefix}Searching the web for:")
            and not turn.startswith(f"{ai_prefix}Generating answers for you...")
        ):
            if turn.startswith(user_prefix):
                turn_num += 1
                value = turn[len(user_prefix) :]

                output.append({"role": "user", "content": value.strip()})

            else:
                value = turn[len(ai_prefix) :]

                output.append({"role": "assistant", "content": value.strip()})

    return output


def convert_convo_to_string(convo):
    return "\n".join([f"{message['role']}: {message['content']}" for message in convo])


async def get_labels_from_convo(convo, prompt):
    split_prompt = prompt.split("PROMPT:")
    main_prompt = split_prompt[1].strip()
    sys_prompt = split_prompt[0].split("SYSTEM:")[1].strip()

    curr_main = main_prompt.replace(
        "{input_conversation}", convert_convo_to_string(convo)
    )

    response = await client.chat.completions.create(
        # model="gpt-4o-2024-05-13", # model = "deployment_name".
        model="gpt-4o",
        # model="gpt-4-0125",
        response_format={"type": "json_object"},
        temperature=0.0,
        max_tokens=4096,  #
        seed=0,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": curr_main},
        ],
    )

    return response.choices[0].message.content


def trim_multiwoz_convo(curr_convo, domain):
    stop_idx = -1
    start_idx = -1
    for idx, turn in enumerate(curr_convo):
        # print the relevant domain

        if turn["frames"]:
            filt_frames = [x for x in turn["frames"] if x["service"] == domain]

            if len(filt_frames) == 0:
                return None, None

            filt_frames = filt_frames[0]

            if (
                len(filt_frames["slots"]) > 0
                or filt_frames["state"]["active_intent"] != "NONE"
            ) and start_idx == -1:
                start_idx = idx

            if (
                "state" in filt_frames
                and filt_frames["state"]["active_intent"] == "NONE"
                and start_idx != -1
                and stop_idx == -1
            ):
                stop_idx = idx

    if start_idx == -1:
        return None, None

    if stop_idx == -1:
        stop_idx = len(curr_convo)

    return start_idx, stop_idx


def filter_data(data, task):
    filtered_convos = []
    filtered_ids = []

    role_map = {"SYSTEM": "assistant", "USER": "user"}

    for convo in data:
        curr_convo = convo["turns"]
        start_idx, stop_idx = trim_multiwoz_convo(curr_convo, task)
        target_utterances = []
        if start_idx is not None:
            for turn in curr_convo[start_idx:stop_idx]:
                target_utterances.append(
                    {"role": role_map[turn["speaker"]], "content": turn["utterance"]}
                )

        if len(target_utterances) > 1:
            filtered_convos.append(target_utterances)
            filtered_ids.append(convo["dialogue_id"])

    return filtered_convos, filtered_ids


def parse_labels(labeled_data, role, full_df=None):
    parsed_labels = []

    def remap_empty(lst, idx):
        lst = [MERGE_MAP[x] for x in lst]

        if idx == 0:
            return ["start"]

        if len(lst) == 0:
            return ["next turn"]

        return lst

    for convo in labeled_data:
        if convo["labels"]:
            convo_id = convo["conversation_id"]
            try:
                q = [
                    remap_empty(x["labels"], idx)
                    for idx, x in enumerate(convo["labels"]["messages"])
                    if x["role"] == role
                ]
            except:
                print("parse fail")
                continue

            q = Counter(list(chain(*q)))

            if "synth" in convo_id:
                domain = "synth"
            else:
                domain = full_df[full_df["ConversationId"] == convo_id].iloc[0][
                    "Domain_v2_Primary"
                ]

            parsed_labels.append(
                {"conversation_id": convo_id, "domain": domain, "labels": q}
            )

    return parsed_labels


# two normalization types:
# total: sum all labels and normalize by that
# convo: normalize by the sum of labels in the conversation, and then divide by number of conversations.
#        This is useful for when you want to see the distribution of labels within a conversation
def normalize_labels(
    parsed_labels, domains, normalization_type="total", always_normalize=True
):
    domain_labels = {domain: Counter() for domain in domains}
    num_convos = {domain: 0 for domain in domains}

    all_labels = Counter()

    for parsed_label in parsed_labels:
        domain = parsed_label["domain"]
        labels = parsed_label["labels"]

        if normalization_type == "total":
            domain_labels[domain] += labels
            all_labels += labels

        elif normalization_type == "convo":
            total_labels = sum(labels.values())
            labels = {k: v / total_labels for k, v in labels.items()}

            domain_labels[domain] += labels
            all_labels += labels

        num_convos[domain] += 1

    if normalization_type == "convo":
        for domain in domain_labels:
            domain_labels[domain] = {
                k: v / num_convos[domain] for k, v in domain_labels[domain].items()
            }

        all_labels = {k: v / len(parsed_labels) for k, v in all_labels.items()}

    # normalize the domain_labels to ensure they sum to 1

    if normalization_type == "total" or always_normalize == True:
        for domain in domain_labels:
            total_labels = sum(domain_labels[domain].values())
            domain_labels[domain] = {
                k: v / total_labels for k, v in domain_labels[domain].items()
            }

        all_labels = {k: v / sum(all_labels.values()) for k, v in all_labels.items()}

    return domain_labels, all_labels
